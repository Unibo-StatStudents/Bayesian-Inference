---
title: "Revision of Lab1"
output: html_document
date: "2023-12-07"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

In this script I will try to better understand the slides and the code of the Lab1 helded by the Tutor D'Agostini.

Let's consider the example proposed in the lab session: Beta-Binomial model.
In the Beta-Binomial model, the Beta prior distribution for the proportion parameter $\theta$ follows a Beta distribution with $a$ and $b$ parameters.
The likelihood function is the following:\\
$y|\theta$ $\thicksim$ $Bin(n, \theta)$
Therefore, as we know from the theory\\
$\theta |y \thicksim Beta(a+r, n-r+b)$ where $r$ are the successes obtained in the $n$ trials.

Now, let's translate it in R code \\

### Prior parameters

We set the initial values for the prior parameters at random

```{r}
a <- 1
b <- 1
```
The sample evidences is the following (those numbers simply recall an example did in class)

```{r}
r <- 15 # number of successes
n <- 20 # number of trials
```
Then, we can say that the posterior parameters are given by the formula written just above the code

```{r}
a_prime <- a + r
b_prime <- n - r + b
```

Now we can sample from the posterior distribution defined before for this particular example. $S$ is the number of samples taken from the Beta distribution.
```{r}
S <- 1e6

# random sample from the posterior
post_sample <- rbeta(n = S, 
                     shape1 = a_prime, 
                     shape2 = b_prime)
```

Now in this plot we can compare how the sampled distribution is similar to the theoretical one.
```{r}
hist(post_sample, breaks = 30, probability = T, 
     main = "Posterior of theta", xlab = "theta")
# theoretical distribution of the posterior
curve(expr = dbeta(x, shape1 = a_prime, shape2 = b_prime), 
      from = 0, to = 1,col = 2, add = T)

```
From the plot we can see that the red line (the theoretical one) is very narrow to the sampled distribution.\\

Now, it follows something that we didn't see in class but, in my opinion helps to understand what we are doing. \\

Here, with the help of course of ChatGpt, I used the shiny package to make an interactive plot where the slider that allows you to interactively change the number of samples. As we increase the number of samples, it's possible to see that the histogram plot becomes more and mores similar to the distribution of the red line.\\


```{r}
# Install and load necessary packages if not already installed
if (!requireNamespace("shiny", quietly = TRUE)) {
  install.packages("shiny")
}

library(shiny)

# Define UI
ui <- fluidPage(
  sliderInput("num_samples", "Number of Samples", min = 100, max = 10000, value = 10000),
  plotOutput("posterior_plot")
)

# Define server logic
server <- function(input, output) {
  observe({
    S <- input$num_samples
    post_sample <- rbeta(n = S, shape1 = a_prime, shape2 = b_prime)

    output$posterior_plot <- renderPlot({
      hist(post_sample, breaks = 30, probability = TRUE,
           main = paste("Posterior of theta (", S, " samples)"), xlab = "theta")
      curve(expr = dbeta(x, shape1 = a_prime, shape2 = b_prime),
            from = 0, to = 1, col = 2, add = TRUE)
    })
  })
}

# Run the Shiny app
shinyApp(ui, server)

```


### Example 2: Normal model

In the first example we showed very briefly how the Monte-Carlo simulation works: although having good properties, Monte Carlo it self is not enough powerful for our porpouses. 
In fact, with high dimensional parameters problems, it is required to introduce the concept of Markov Chains: the reader who wants to go deep in the theory of this particular stochastic process, can find in the Unibo-StatStudents a repository called "Stochastic process" which can be useful.

In this example we are dealing with the conjugate Normal model: we recall that for the normal distribution the conjugate prior is a normal distribution for the mean and a gamma distribution for the precision (or equivalently, the variance).

In the slides you can find a better formalization of the problem, I will try to stay consistent with the notation.


Let's set the hyperparameters for the Normal and the Gamma distribution
```{r}
# Inverse gamma prior for variance: hyperparameters
nu_0 <- 1
S_0 <- 1

# Normal prior for the mean: hyperparameters
theta_0 <- 0
phi_0 <- 100
```

And now let's generate a sample from the normal distribution

```{r}
n <- 40

set.seed(123)
y <- rnorm(n = n, mean = 1, sd = 1)

mean(y)
var(y)
```
Moreover, we know that the posterior parameters are defined as

```{r}
a_1 <- nu_0 / 2 + n / 2
```

Now we implement the Gibbs sampler: first thing to do is to define the parameter space.


```{r}
B <- 1e4 #number of iterations
# Create vector for realizations
phi_sample <- numeric(B) #variance
theta_sample <- numeric(B) #mean
```
Then we fix the initial state of the parameters

```{r}
phi_sample[1] <- 1
theta_sample[1] <- 0
```

Then we generate the samples in the Gibbs fashion with this for loop

```{r}
for (i in 2:B) {
  theta_1_den  <- (sum(y) / phi_sample[i - 1] + theta_0 / phi_0)
  phi_1  <- (n / phi_sample[i - 1] + 1 / phi_0) ^ (-1)
  theta_sample[i] <- rnorm(1, theta_1_den * phi_1, sqrt(phi_1))

  b_1  <- S_0 / 2 + sum((y - theta_sample[i]) ^ 2) / 2
  phi_sample[i] <- 1 / rgamma(1, a_1, b_1)
}
```

Now it's time to make some visualization:

Here we can see the joint distribution of ($\phi$ and $\theta$)

```{r}
plot(theta_sample, phi_sample, xlab = "theta", 
     ylab = "phi", main = "Joint posterior distribution")
```
Here, the posteriors distributions of the parameters
```{r}
# Marginal posteriors
hist(phi_sample, breaks = 30, xlab = "phi", main = "Posterior of phi")
mean(phi_sample); sd(phi_sample)
quantile(phi_sample, probs = c(0.025, 0.25, 0.5, 0.75, 0.095))

hist(theta_sample, breaks = 30, xlab = "theta", main = "Posterior of theta")
mean(theta_sample); sd(theta_sample)
quantile(theta_sample, probs = c(0.025, 0.25, 0.5, 0.75, 0.095))

```
Here, the Markov Chains of the two parameters
```{r}
# Markov chains
plot(phi_sample, ylab = "phi", xlab = "Iteration", type = "l")
plot(theta_sample, ylab = "theta", xlab = "Iteration", type = "l")

```
From the two plots we can notice that both parameters quickly converge to the posteriors values.


